# ============== Configuration file for SoftMatch + SimCLR training ==============

# ----------- Shared settings -------------
model_saved_path: "model_checkpoints/" # path to save model checkpoints

# Models to train
models: ["resnet18"] # list of encoder backbones to use - any of ["resnet50", "vit_b_16", "efficientnet_b5"]

# Training
seed: 42 
image_size: 96
fold: 0 # Which 1k fold to use (0-9). Set to null to use all 5k labels

## Pre-training options
# NOTE: batch_size is TOTAL effective batch size (will be divided by num_gpus)
pretrain_batch_size: 512 
pretrain_learning_rate: 0.1
pretrain_epochs: 400
pretrain_weight_decay: 1e-4
pretrain_momentum: 0.9
pretrain_temperature: 0.5

## Logging and saving options
eval_every: 50 # evaluate model performance every N epochs during pre-training
save_model_every: 100 # save model checkpoint every N epochs during pre-training (must be multiple of eval_every)


# ---------- SoftMatch options ----------

softmatch_sup_weight: 1.0 # weight for supervised loss
softmatch_unsup_weight: 1.0 # weight for unsupervised consistency loss
simclr_weight: 5.0 # weight for SimCLR loss in combined loss function 

softmatch_dist_align: true # whether to apply distribution alignment
softmatch_hard_label: true # use hard pseudo-labels (SoftMatch paper default = true)
softmatch_ema_p: 0.999 # EMA decay for probability tracking
softmatch_model_ema: 0.999 # EMA decay for model weights

soft_weights_epoch: 50 # epoch to start applying soft weights to unsupervised loss
softmatch_loss_weight: 1.0 # multiplier for softmatch loss in loss = simclr_loss + softmatch_loss_weight * softmatch_loss

use_decaying_loss_weight: false # whether to use decaying loss weight for unsupervised loss
decaying_weight_min: 0.2 # minimum weight for decaying loss weight
decaying_weight_max: 0.5 # maximum weight for decaying loss weight
