# ============== Configuration file for SoftMatch + SimCLR training ==============

# ----------- Shared settings -------------
model_saved_path: "model_checkpoints/" # path to save model checkpoints

# Models to train
models: ["resnet50"] # list of encoder backbones to use - any of ["resnet50", "vit_b_16", "efficientnet_b5"]

# Training
seed: 42 
image_size: 224

## Pre-training options
# NOTE: batch_size is TOTAL effective batch size (will be divided by num_gpus)
pretrain_batch_size: 512 
pretrain_learning_rate: 0.1
pretrain_epochs: 400
pretrain_weight_decay: 1e-4
pretrain_momentum: 0.9
pretrain_temperature: 0.5

## Logging and saving options
eval_every: 50 # evaluate model performance every N epochs during pre-training
save_model_every: 100 # save model checkpoint every N epochs during pre-training (must be multiple of eval_every)


# ---------- SoftMatch options ----------
softmatch_subset_ratio: 0.1 # ratio of labeled data used for supervised loss (0.1 means 10% of the training data)
softmatch_sup_weight: 1.0 # weight for supervised loss
softmatch_unsup_weight: 10.0 # weight for unsupervised consistency loss
simclr_loss_weight: 2.0 # weight for SimCLR loss in total loss

softmatch_dist_align: true # whether to apply distribution alignment
softmatch_hard_label: true # use hard pseudo-labels (SoftMatch paper default = true)
softmatch_ema_p: 0.999 # EMA decay for probability tracking
softmatch_model_ema: 0.999 # EMA decay for model weights

soft_weights_epoch: 50 # epoch to start applying soft weights to unsupervised loss
