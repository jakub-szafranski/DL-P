# ============== Configuration file for SimCLR training ==============

# ----------- Shared settings -------------
model_saved_path: "model_checkpoints/" # path to save model checkpoints

# Models to train
models: ["resnet50"] # list of encoder backbones to use - any of ["resnet50", "vit_b_16", "efficientnet_b5"]

# Training
seed: 42 
image_size: 96

## Pre-training options
# NOTE: batch_size is TOTAL effective batch size (will be divided by num_gpus)
pretrain_batch_size: 512 
pretrain_learning_rate: 0.6 # [...] optimized using LARS with learning rate of 4.8 (= 0.3 ×BatchSize/256)
pretrain_epochs: 400
pretrain_weight_decay: 1e-6 # "optimized using LARS [...] and weight decay of 10−6"
pretrain_temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

## Logging and saving options
eval_every: 50 # evaluate model performance every N epochs during pre-training
save_model_every: 100 # save model checkpoint every N epochs during pre-training (must be multiple of eval_every)


# ---------- Fine-tuning options ----------

# Stage 1: Frozen encoder - train only classifier head
ft_frozen_batch_size: 512
ft_frozen_learning_rate: 0.02 # LearningRate = 0.1 × BatchSize/256
ft_frozen_epochs: 30
ft_frozen_momentum: 0.9

# Stage 2: Full fine-tuning - unfreeze encoder and train entire model
ft_full_batch_size: 512
ft_full_learning_rate: 0.01 #  LearningRate = 0.05 ×BatchSize/256
ft_full_epochs: 60 
ft_full_momentum: 0.9

eval_model_paths: [
  "model_checkpoints/simclr_resnet50_epoch_100.pth",
  "model_checkpoints/simclr_resnet50_epoch_200.pth",
  "model_checkpoints/simclr_resnet50_epoch_300.pth",
  "model_checkpoints/simclr_resnet50_epoch_400.pth"
  ] # For simclr_eval.py only: list of model checkpoint paths to evaluate
